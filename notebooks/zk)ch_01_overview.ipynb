{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63badeb3-860a-4a42-ba35-790fd99c5e80",
   "metadata": {},
   "source": [
    "# An Overview of Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755b31b",
   "metadata": {},
   "source": [
    "One of the reasons we need efficient distributed computing is that we’re collecting\n",
    "ever more data with a large variety at increasing speeds. The storage systems, data\n",
    "processing and analytics engines that have emerged in the last decade are crucially\n",
    "important to the success of many companies. Interestingly, most “big data” technologies\n",
    "are built for and operated by (data) engineers, that are in charge of data\n",
    "collection and processing tasks. The rationale is to free up data scientists to do\n",
    "what they’re best at. As a data science practitioner you might want to focus on training\n",
    "complex machine learning models, running efficient hyperparameter selection,\n",
    "building entirely new and custom models or simulations, or serving your models to\n",
    "showcase them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cebcd0",
   "metadata": {},
   "source": [
    "At the same time, it might be inevitable to scale these workloads to a compute\n",
    "cluster. To do that, the distributed system of your choice needs to support all of these\n",
    "fine-grained “big compute” tasks, potentially on specialized hardware. Ideally, it also\n",
    "fits into the big data tool chain you’re using and is fast enough to meet your latency\n",
    "requirements. In other words, distributed computing has to be powerful and flexible\n",
    "enough for complex data science workloads — and Ray can help you with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00480805",
   "metadata": {},
   "source": [
    "Python is likely the most popular language for data science today, and it’s certainly\n",
    "the one we find the most useful for our daily work. By now it’s over 30 years old,\n",
    "but has a still growing and active community. The rich [PyData ecosystem](https://\n",
    "pydata.org/) is an essential part of a data scientist’s toolbox. How can you make sure\n",
    "to scale out your workloads while still leveraging the tools you need? That’s a difficult\n",
    "problem, especially since communities can’t be forced to just toss their toolbox, or\n",
    "programming language. That means distributed computing tools for data science\n",
    "have to be built for their existing community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2dc7e",
   "metadata": {},
   "source": [
    "Every chapter of this book has an executable notebook that you can run. If you want run the code while following this chapter, you can run this notebook locally or directly in [Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_01_overview.ipynb): \n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_01_overview.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7dd72",
   "metadata": {},
   "source": [
    "For this chapter you need to install the following dependencies. We guide you through each dependency and when you need it later on, but if you're impatient you can install them all right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d03d5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray==2.2.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray[rllib,serve,tune]==2.2.0) (2.2.0)\n",
      "Requirement already satisfied: attrs in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (23.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (8.1.7)\n",
      "Requirement already satisfied: filelock in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (3.15.3)\n",
      "Requirement already satisfied: jsonschema in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (4.22.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (1.0.8)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (5.27.1)\n",
      "Requirement already satisfied: pyyaml in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (6.0.1)\n",
      "Requirement already satisfied: aiosignal in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (1.4.1)\n",
      "Requirement already satisfied: requests in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (2.32.3)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (20.26.3)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (1.64.1)\n",
      "Requirement already satisfied: packaging in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (24.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (2.0.0)\n",
      "Collecting pandas (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting tabulate (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tensorboardX>=1.9 (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting smart-open (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting aiohttp>=3.7 (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting aiorwlock (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading aiorwlock-1.4.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting colorful (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pydantic (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading pydantic-2.7.4-py3-none-any.whl.metadata (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m672.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting starlette (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting uvicorn (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting fastapi (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting gpustat>=1.0.0 (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading gpustat-1.1.1.tar.gz (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m828.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting prometheus-client<0.14.0,>=0.7.1 (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading prometheus_client-0.13.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting aiohttp-cors (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting py-spy>=0.2.0 (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
      "Collecting opencensus (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dm-tree (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting gym<0.24.0,>=0.21.0 (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m731.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lz4 (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting matplotlib!=3.4.3 (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scikit-image (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading scikit_image-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting scipy (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typer (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting rich (from ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.7->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.7->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp>=3.7->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting nvidia-ml-py>=11.450.129 (from gpustat>=1.0.0->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading nvidia_ml_py-12.555.43-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: psutil>=5.6.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[rllib,serve,tune]==2.2.0) (6.0.0)\n",
      "Collecting blessed>=1.17.1 (from gpustat>=1.0.0->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading blessed-1.20.0-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cloudpickle>=1.2.0 (from gym<0.24.0,>=0.21.0->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym<0.24.0,>=0.21.0->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib!=3.4.3->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib!=3.4.3->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib!=3.4.3->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m172.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib!=3.4.3->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=8 (from matplotlib!=3.4.3->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib!=3.4.3->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from matplotlib!=3.4.3->ray[rllib,serve,tune]==2.2.0) (2.9.0.post0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (0.3.8)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from virtualenv>=20.0.24->ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (4.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from fastapi->ray[rllib,serve,tune]==2.2.0) (4.12.2)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from fastapi->ray[rllib,serve,tune]==2.2.0) (0.27.0)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from fastapi->ray[rllib,serve,tune]==2.2.0) (3.1.4)\n",
      "Collecting python-multipart>=0.0.7 (from fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "Collecting orjson>=3.2.1 (from fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m80.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.4 (from pydantic->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from starlette->ray[rllib,serve,tune]==2.2.0) (4.4.0)\n",
      "Requirement already satisfied: h11>=0.8 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from uvicorn->ray[rllib,serve,tune]==2.2.0) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from jsonschema->ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from jsonschema->ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from jsonschema->ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (0.18.1)\n",
      "Collecting opencensus-context>=0.1.3 (from opencensus->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: six~=1.16 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from opencensus->ray[rllib,serve,tune]==2.2.0) (1.16.0)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0 (from opencensus->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests->ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests->ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests->ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests->ray==2.2.0->ray[rllib,serve,tune]==2.2.0) (2024.6.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->ray[rllib,serve,tune]==2.2.0)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from rich->ray[rllib,serve,tune]==2.2.0) (2.18.0)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from scikit-image->ray[rllib,serve,tune]==2.2.0) (3.3)\n",
      "Collecting imageio>=2.33 (from scikit-image->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading tifffile-2024.6.18-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting wrapt (from smart-open->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer->ray[rllib,serve,tune]==2.2.0)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->ray[rllib,serve,tune]==2.2.0) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->ray[rllib,serve,tune]==2.2.0) (1.2.1)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[rllib,serve,tune]==2.2.0) (0.2.13)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3 (from ray==2.2.0->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading google_auth-2.30.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi->ray[rllib,serve,tune]==2.2.0) (1.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi->ray[rllib,serve,tune]==2.2.0) (2.1.5)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->ray[rllib,serve,tune]==2.2.0)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0->fastapi->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[rllib,serve,tune]==2.2.0)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m206.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m276.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m249.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m184.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m148.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading aiorwlock-1.4.0-py3-none-any.whl (10.0 kB)\n",
      "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m275.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m333.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m295.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.4-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.0/409.0 kB\u001b[0m \u001b[31m302.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m245.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m220.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m227.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m182.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m275.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m230.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m230.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_image-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m289.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m282.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m171.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m196.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m280.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m322.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Downloading fonttools-4.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m251.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m249.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m298.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Downloading imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m277.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m301.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m109.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.555.43-py3-none-any.whl (39 kB)\n",
      "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m180.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m208.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m153.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m220.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading tifffile-2024.6.18-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m193.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m170.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m85.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m137.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m122.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m141.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m149.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.2/229.2 kB\u001b[0m \u001b[31m141.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m180.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m172.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m162.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m204.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m170.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m101.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m184.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: gpustat, gym\n",
      "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpustat: filename=gpustat-1.1.1-py3-none-any.whl size=26535 sha256=85f908d0d745094e1cd8fb5bf3887e257ff73cddad35c2431426d7c9cd1c76f1\n",
      "  Stored in directory: /home/kython/.cache/pip/wheels/ec/d7/80/a71ba3540900e1f276bcae685efd8e590c810d2108b95f1e47\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.23.1-py3-none-any.whl size=701343 sha256=b3f74b95b3fe840cb0844eb004924964eefdb6b27c21fc8d095c13deecd16a76\n",
      "  Stored in directory: /home/kython/.cache/pip/wheels/1a/00/fb/fe5cf2860fb9b7bc860e28f00095a1f42c7b726dd6f42d1acc\n",
      "Successfully built gpustat gym\n",
      "Installing collected packages: pytz, py-spy, opencensus-context, nvidia-ml-py, gym-notices, dm-tree, colorful, wrapt, websockets, uvloop, uvicorn, ujson, tzdata, tifffile, tabulate, shellingham, scipy, python-multipart, python-dotenv, pyparsing, pydantic-core, pyasn1, protobuf, prometheus-client, pillow, orjson, multidict, mdurl, lz4, lazy-loader, kiwisolver, httptools, fonttools, dnspython, cycler, contourpy, cloudpickle, cachetools, blessed, async-timeout, annotated-types, aiorwlock, yarl, watchfiles, tensorboardX, starlette, smart-open, rsa, pydantic, pyasn1-modules, proto-plus, pandas, matplotlib, markdown-it-py, imageio, gym, gpustat, googleapis-common-protos, email_validator, scikit-image, rich, google-auth, aiohttp, typer, google-api-core, aiohttp-cors, opencensus, fastapi-cli, fastapi\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.27.1\n",
      "    Uninstalling protobuf-5.27.1:\n",
      "      Successfully uninstalled protobuf-5.27.1\n",
      "  Attempting uninstall: prometheus-client\n",
      "    Found existing installation: prometheus_client 0.20.0\n",
      "    Uninstalling prometheus_client-0.20.0:\n",
      "      Successfully uninstalled prometheus_client-0.20.0\n",
      "Successfully installed aiohttp-3.9.5 aiohttp-cors-0.7.0 aiorwlock-1.4.0 annotated-types-0.7.0 async-timeout-4.0.3 blessed-1.20.0 cachetools-5.3.3 cloudpickle-3.0.0 colorful-0.5.6 contourpy-1.2.1 cycler-0.12.1 dm-tree-0.1.8 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 fonttools-4.53.0 google-api-core-2.19.0 google-auth-2.30.0 googleapis-common-protos-1.63.1 gpustat-1.1.1 gym-0.23.1 gym-notices-0.0.8 httptools-0.6.1 imageio-2.34.1 kiwisolver-1.4.5 lazy-loader-0.4 lz4-4.3.3 markdown-it-py-3.0.0 matplotlib-3.9.0 mdurl-0.1.2 multidict-6.0.5 nvidia-ml-py-12.555.43 opencensus-0.11.4 opencensus-context-0.1.3 orjson-3.10.5 pandas-2.2.2 pillow-10.3.0 prometheus-client-0.13.1 proto-plus-1.24.0 protobuf-4.25.3 py-spy-0.3.14 pyasn1-0.6.0 pyasn1-modules-0.4.0 pydantic-2.7.4 pydantic-core-2.18.4 pyparsing-3.1.2 python-dotenv-1.0.1 python-multipart-0.0.9 pytz-2024.1 rich-13.7.1 rsa-4.9 scikit-image-0.24.0 scipy-1.13.1 shellingham-1.5.4 smart-open-7.0.4 starlette-0.37.2 tabulate-0.9.0 tensorboardX-2.6.2.2 tifffile-2024.6.18 typer-0.12.3 tzdata-2024.1 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0 wrapt-1.16.0 yarl-1.9.4\n",
      "Collecting pyarrow==10.0.0\n",
      "  Downloading pyarrow-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from pyarrow==10.0.0) (2.0.0)\n",
      "Downloading pyarrow-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m260.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-10.0.0\n",
      "Collecting tensorflow>=2.9.0\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow>=2.9.0)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.9.0)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow>=2.9.0)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.9.0)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow>=2.9.0)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow>=2.9.0)\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow>=2.9.0)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow>=2.9.0)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow>=2.9.0)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from tensorflow>=2.9.0) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from tensorflow>=2.9.0) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from tensorflow>=2.9.0) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from tensorflow>=2.9.0) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from tensorflow>=2.9.0) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=2.9.0)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from tensorflow>=2.9.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from tensorflow>=2.9.0) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from tensorflow>=2.9.0) (1.64.1)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow>=2.9.0)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow>=2.9.0)\n",
      "  Downloading keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow>=2.9.0)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow>=2.9.0)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m576.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.9.0) (0.43.0)\n",
      "Requirement already satisfied: rich in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow>=2.9.0) (13.7.1)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow>=2.9.0)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow>=2.9.0)\n",
      "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m146.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow>=2.9.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow>=2.9.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow>=2.9.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow>=2.9.0) (2024.6.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow>=2.9.0)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow>=2.9.0)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow>=2.9.0)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow>=2.9.0) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow>=2.9.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow>=2.9.0) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow>=2.9.0) (0.1.2)\n",
      "Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m655.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:24\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m720.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m681.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m968.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m634.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m670.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m809.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m703.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m851.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m729.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m632.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m297.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m362.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, numpy, markdown, google-pasta, gast, astunparse, absl-py, tensorboard, opt-einsum, ml-dtypes, h5py, keras, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 h5py-3.11.0 keras-3.3.3 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 namex-0.0.8 numpy-1.26.4 opt-einsum-3.3.0 optree-0.11.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 werkzeug-3.0.3\n",
      "Collecting transformers>=4.24.0\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m403.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from transformers>=4.24.0) (3.15.3)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers>=4.24.0)\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from transformers>=4.24.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from transformers>=4.24.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from transformers>=4.24.0) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.24.0)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m480.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from transformers>=4.24.0) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.24.0)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.24.0)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers>=4.24.0)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m177.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers>=4.24.0) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers>=4.24.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests->transformers>=4.24.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests->transformers>=4.24.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests->transformers>=4.24.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from requests->transformers>=4.24.0) (2024.6.2)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m800.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m951.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m682.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m809.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m708.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m223.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.4 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.41.2\n",
      "Collecting pygame==2.1.2\n",
      "  Downloading pygame-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting gym==0.25.0\n",
      "  Downloading gym-0.25.0.tar.gz (720 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m924.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from gym==0.25.0) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from gym==0.25.0) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/kython/miniconda3/envs/rayenv/lib/python3.10/site-packages (from gym==0.25.0) (0.0.8)\n",
      "Downloading pygame-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.9/21.9 MB\u001b[0m \u001b[31m926.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.25.0-py3-none-any.whl size=824403 sha256=aafc45468fb26d00bbbe04527981e388c975aa6c22ab9805af3f5861362f262d\n",
      "  Stored in directory: /home/kython/.cache/pip/wheels/c0/3c/33/32d86254a5bd554f5f07759ae1794646e490dd5fa81ebdcda3\n",
      "Successfully built gym\n",
      "Installing collected packages: pygame, gym\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.23.1\n",
      "    Uninstalling gym-0.23.1:\n",
      "      Successfully uninstalled gym-0.23.1\n",
      "Successfully installed gym-0.25.0 pygame-2.1.2\n"
     ]
    }
   ],
   "source": [
    "! pip install \"ray[rllib, serve, tune]==2.2.0\"\n",
    "! pip install \"pyarrow==10.0.0\"\n",
    "! pip install \"tensorflow>=2.9.0\"\n",
    "! pip install \"transformers>=4.24.0\"\n",
    "! pip install \"pygame==2.1.2\" \"gym==0.25.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2050f",
   "metadata": {},
   "source": [
    "To import utility files for this chapter on Colab you will also have to clone the repo and copy the code files to the base path of the runtime. You don't need to do this if you run the notebook locally, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafdce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'learning_ray'...\n",
      "remote: Enumerating objects: 1385, done.\u001b[K\n",
      "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
      "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
      "remote: Total 1385 (delta 41), reused 45 (delta 19), pack-reused 1313\u001b[K\n",
      "Receiving objects: 100% (1385/1385), 119.42 MiB | 994.00 KiB/s, done.\n",
      "Resolving deltas: 100% (755/755), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/maxpumperla/learning_ray\n",
    "%cp -r learning_ray/notebooks/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a63e",
   "metadata": {},
   "source": [
    "## What is Ray?\n",
    "\n",
    "Ray is a great computing framework for the Python data science community because it is flexible and distributed, making it easy to use and understand. It allows you to efficiently parallelize Python programs on your own computer and run them on a cluster without much modification. Additionally, its high-level libraries are easy to set up and can be used together smoothly, and some of them, such as the reinforcement learning library, have a promising future as standalone projects. Even though its core is written in C++, Ray has always been focused on Python and integrates well with many important data science tools. It also has a expanding ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795bed4",
   "metadata": {},
   "source": [
    "Ray is not the first framework for distributed Python, nor will it be the last, but it stands out for its ability to handle custom machine learning tasks with ease. Its various modules work well together, allowing for the flexible execution of complex workloads using familiar Python tools. This book aims to teach how to use Ray to effectively utilize distributed Python for machine learning purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba089d",
   "metadata": {},
   "source": [
    "Programming distributed systems can be challenging because it requires specific skills and experience. While these systems are designed to be efficient and allow users to focus on their tasks, they often have [\"leaky abstractions\"](https://www.joelonsoftware.com/2002/11/11/thelaw-of-leaky-abstractions) that can make it difficult to get clusters of computers to work as desired. In addition, many software systems require more resources than a single server can provide, and modern systems need to be able to handle failures and offer high availability. This means that applications may need to run on multiple machines or even in different data centers in order to function reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08256e35",
   "metadata": {},
   "source": [
    "Even if you are not very familiar with machine learning (ML) or artificial intelligence (AI), you have probably heard about recent advances in these fields. Some examples of these advances include Deepmind's Alpha-Fold, which is a system for solving the protein folding problem, and OpenAI's Codex, which helps software developers with the tedious parts of their job. It is commonly known that ML systems require a lot of data to be trained and that ML models tend to become larger. OpenAI has demonstrated that the amount of computing power needed to train AI models has been increasing exponentially, as shown in their paper \"AI and Compute.\" In their study, the operations needed for AI systems were measured in petaflops (thousands of trillion operations per second) and have doubled every 3.4 months since 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a010dd",
   "metadata": {},
   "source": [
    "While Moore's Law suggests that computer transistors will double every two years, the use of distributed computing in machine learning can significantly increase the speed at which tasks are completed. While distributed computing may be seen as challenging, it would be beneficial to develop abstractions that allow for code to run on clusters without constantly considering individual machines and their interactions. By focusing specifically on AI workloads, it may be possible to make distributed computing more accessible and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1fcd87",
   "metadata": {},
   "source": [
    "Researchers at RISELab at UC Berkeley developed Ray to improve the efficiency of their workloads by distributing them. These workloads were flexible in nature and did not fit into existing frameworks. Ray was also designed to handle the distribution of the work and allow researchers to focus on their work without worrying about the specifics of their compute cluster. It was created with a focus on high-performance and diverse workloads, and allows researchers to use their preferred Python tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28aaa68",
   "metadata": {},
   "source": [
    "### Design Philosophy\n",
    "\n",
    "Ray was created with a focus on certain design principles. Its API aims to be straightforward and applicable to a wide range of situations, while the compute model is designed to be adaptable. Additionally, the system architecture is optimized for speed and the ability to handle increasing workloads. Let's delve into these points further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dea857",
   "metadata": {},
   "source": [
    "#### Simplicity and abstraction\n",
    "\n",
    "Ray's API is not only simple to use, but it is also intuitive and easy to learn, as you will see in Chapter 2. Whether you want to use all the CPU cores on your laptop or leverage all the machines in your cluster, you can do so with minimal changes to your code. Ray handles task distribution and coordination behind the scenes, allowing you to focus on your work rather than worrying about the mechanics of distributed computing. Additionally, the API is very flexible and can easily be integrated with other tools. For example, Ray actors can interact with other distributed Python workloads, making it a useful \"glue code\" for connecting different systems and frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f1f1a",
   "metadata": {},
   "source": [
    "#### Flexibility and heterogeneity\n",
    "\n",
    "Ray's API is created to allow users to easily write flexible and modular code for artificial intelligence tasks, especially those involving reinforcement learning. As long as the workload can be expressed in Python, it can be distributed using Ray. However, it is important to ensure that sufficient resources are available and to consider what should be distributed. Ray does not impose any limitations on what can be done with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83621e6c",
   "metadata": {},
   "source": [
    "Ray is able to handle a variety of different computational tasks. For example, when working on a complex simulation, it is common for there to be different steps that take different amounts of time to complete. Some may take a long time while others only take a few milliseconds. Ray is able to efficiently schedule and execute these tasks, even if they need to be run in parallel. Additionally, Ray's framework allows for dynamic execution, which is helpful when subsequent tasks depend on the outcome of an earlier task. Overall, Ray provides flexibility in managing heterogeneous workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637aa16",
   "metadata": {},
   "source": [
    "It is important to be able to adapt your resource usage and Ray allows for the use of different types of hardware. For example, certain tasks may require the use of a GPU while others may perform better on a CPU. Ray gives you the ability to choose the most appropriate hardware for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752190b",
   "metadata": {},
   "source": [
    "#### Speed and scalability\n",
    "\n",
    "One of the key features of Ray is its speed. It can handle millions of tasks per second with minimal latency, making it an efficient choice for distributed systems. Additionally, Ray is effective at distributing and scheduling tasks across a compute cluster, and it does so in a way that is fault-tolerant. Its auto-scaler can adjust the number of machines in the cluster to match current demand, which helps to minimize costs and ensure there are enough resources available to run workloads. In the event of failures, Ray is designed to recover quickly, further contributing to its overall speed. While we will delve into the specifics of Ray's architecture later on, for now, let's focus on how it can be used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cf37d6",
   "metadata": {},
   "source": [
    "### Core, Libraries and Ecosystem\n",
    "\n",
    "Now that you are aware of the purpose and goals behind the creation of Ray, let's examine the three layers of the system. While there may be other ways to classify these layers, the approach used in this book is the most logical and understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe95e4",
   "metadata": {},
   "source": [
    "1. A low-level, distributed computing framework for Python with a concise core API and tooling for cluster deployment called Ray Core.\n",
    "2. A set of high-level libraries for built and maintained by the creators of Ray. This includes the so-called Ray AI Runtime (AIR) to use these libraries with a unified API in common machine learning workloads.\n",
    "3. A growing ecosystem of integrations and partnerships with other notable\n",
    "projects, which span many aspects of the first two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45267d",
   "metadata": {},
   "source": [
    "There are several layers to explore in this chapter. The core of Ray's engine, with its API at the center, serves as the foundation for everything else. The data science libraries in Ray build on top of this core and offer a specialized interface. Many data scientists will use these libraries directly, while those working in ML or platform engineering may focus on creating tools that extend the Ray Core API. Ray AIR serves as a connector between the various Ray libraries and provides a consistent framework for handling common AI tasks. Additionally, there are a increasing number of third-party integrations available for Ray that can be utilized by experienced practitioners. We will examine each of these layers in more detail.\n",
    "\n",
    "Below is a quick preview of what libraries and integrations each layer consists of. Maybe you already spot a few of your favorite tools from the ecosystem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551fa5c",
   "metadata": {},
   "source": [
    "![Ray Layers](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_01/ray_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4462377",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## A framework for distributed computing\n",
    "\n",
    "At its core, Ray is a distributed computing framework.\n",
    "We'll  provide you with just the basic terminology here, and talk about Ray's\n",
    "architecture in depth in chapter 2.\n",
    "In short, Ray sets up and manages clusters of computers so that you can run\n",
    "distributed tasks on them.\n",
    "A ray cluster consists of nodes that are connected to each other via a network.\n",
    "You program against the so-called _driver_, the program root, which lives on\n",
    "the _head node_.\n",
    "The driver can run _jobs_, that is a collection of tasks, that are run on the nodes\n",
    "in the cluster.\n",
    "Specifically, the individual tasks of a job are run on _worker_ processes on\n",
    "worker nodes_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c2640",
   "metadata": {},
   "source": [
    "![Ray cluster](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_01/simple_cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668e1ae",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "What's interesting is that a Ray cluster can also be a _local cluster_, i.e. a cluster\n",
    "consisting just of your own computer.\n",
    "In this case, there's just one node, namely the head node, which has the driver\n",
    "process and some worker processes.\n",
    "\n",
    "With that knowledge at hand, it's time to get your hands dirty and run your first\n",
    "local Ray cluster.\n",
    "Installing Ray on any of the major operating systems should work seamlessly\n",
    "using `pip`:\n",
    "\n",
    "```\n",
    "pip install \"ray[rllib, tune, serve]==2.2.0\"\n",
    "```\n",
    "\n",
    "With a simple `pip install ray` you would have installed just the very basics of Ray.\n",
    "Since we want to explore some advanced features, we installed the \"extras\" `rllib`\n",
    "and `tune`, which we'll discuss in a bit.\n",
    "Depending on your system configuration you may not need the quotation marks in the\n",
    "above installation command.\n",
    "\n",
    "Next, go ahead and start a Python session.\n",
    "You could use the `ipython` interpreter, which I find to be the most suitable\n",
    "environment for following along simple examples.\n",
    "The choice is up to you, but in any case please remember to use Python version\n",
    "`3.7` or later.\n",
    "In your Python session you can now easily import and initialize Ray as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29d6ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 13:49:31,554\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.14</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.2.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.10.14', ray_version='2.2.0', ray_commit='b6af0887ee5f2e460202133791ad941a41f15beb', address_info={'node_ip_address': '192.168.1.11', 'raylet_ip_address': '192.168.1.11', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2024-06-23_13-49-28_946002_9468/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2024-06-23_13-49-28_946002_9468/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2024-06-23_13-49-28_946002_9468', 'metrics_export_port': 64077, 'gcs_address': '192.168.1.11:57523', 'address': '192.168.1.11:57523', 'dashboard_agent_listen_port': 52365, 'node_id': '094690fc7577c4344a8494a067096a6a6506906aef0275e89602fefa'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de3a35",
   "metadata": {},
   "source": [
    "By running those two lines of code, you have set up a Ray cluster on your local machine. This cluster can take advantage of all the cores on your computer as worker processes. Currently, your Ray cluster isn't doing much, but that will change in the following section. The init function used to initiate the cluster is one of just six fundamental API calls that you will delve into in Chapter 2. Overall, the Ray Core API is straightforward and easy to use, but since it is also a lower-level interface, it takes time to create more complex examples with it. Chapter 2 includes a detailed first example to introduce you to the Ray Core API, and in Chapter 3 you will see how to build a more advanced Ray application for reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfedfee",
   "metadata": {},
   "source": [
    "In the above example, you did not provide any arguments when calling the `ray.init(...)` function. If you wanted to use Ray on a real cluster, you would need to include more arguments in this init call, which is known as the Ray Client. The Ray Client is used to connect to an existing Ray cluster and interact with it. If you are interested in learning more about using the Ray Client to connect to your production clusters, you can refer to the [Ray documentation](https://docs.ray.io/en/latest/cluster/ray-client.html). Keep in mind that working with compute clusters can be complex, and there are many options for deploying Ray applications on them. For example, you can use cloud providers like AWS, GCP, or Azure to host your Ray clusters, or you can set up your own hardware or use tools like Kubernetes. We will revisit the topic of scaling workloads with Ray Clusters in Chapter 9, after discussing some specific applications of Ray in earlier chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616cab30",
   "metadata": {},
   "source": [
    "Before moving on the Ray’s higher level libraries, let’s briefly summarize the two\n",
    "foundational components of Ray as a distributed computation framework:\n",
    "\n",
    "- _Ray Clusters_: This component is in charge of allocating resources, creating nodes,\n",
    "and ensuring they are healthy. A good way to get started with Ray Clusters is its\n",
    "dedicated quick start guide (https://docs.ray.io/en/latest/cluster/quickstart.html).\n",
    "- _Ray Core_: Once your cluster is up and running, you use the Ray Core API\n",
    "that to program against it. You can get started with Ray Core by following the\n",
    "official walk-through (https://docs.ray.io/en/latest/ray-core/walkthrough.html) for\n",
    "this component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c684724",
   "metadata": {},
   "source": [
    "## Ray's Libraries\n",
    "\n",
    "In this section, we will introduce the data science libraries included with Ray. To understand how these libraries can be beneficial to you, it's important to first have a general understanding of what data science entails. With this context in mind, you'll be able to see how Ray's higher-level libraries fit into the larger picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5e6fb",
   "metadata": {},
   "source": [
    "### Ray AIR and the Data Science Workflow\n",
    "\n",
    "The concept of \"data science\" (DS) has undergone significant changes in recent years, and you can find various definitions of the term online, some more useful than others. However, we believe that data science is the practice of using data to gain insights and develop practical applications. It is a field that involves building and understanding things, and is therefore quite practical and applied. In this sense, calling practitioners of this field \"data scientists\" is similar to calling hackers \"computer scientists\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c96d8",
   "metadata": {},
   "source": [
    "Data science involves a series of steps that involve identifying and gathering the necessary data, processing it, creating models, and implementing solutions. While machine learning may be a part of this process, it is not always necessary. If machine learning is included, there may be additional steps involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925fab2",
   "metadata": {},
   "source": [
    "- _Data Processing_: To use machine learning effectively, you must prepare the data in a way that the ML model can understand. This process, called feature engineering, involves selecting and transforming the data that will be input into the model. It can be a challenging task, so it is helpful to have access to reliable tools to assist with it.\n",
    "- _Model Training_: For machine learning, it is necessary to train your algorithms on data that has been previously processed. This involves selecting the appropriate algorithm for the task at hand. Having a diverse range of algorithms to choose from can also be beneficial.\n",
    "- _Hyperparameter Tuning_: During the process of training a machine learning model, certain parameters can be fine-tuned in order to improve its performance. In addition to these model parameters, there are also hyperparameters that can be adjusted before training begins. The proper adjustment of these hyperparameters can significantly impact the effectiveness of the final machine learning model. Fortunately, there are tools available to assist with the process of optimizing these hyperparameters.\n",
    "- _Model Serving_: The deployment of trained models is necessary in order to provide access to them for those who need it. This process, known as serving a model, involves making it available through various means, such as using simple HTTP servers in prototypes or specialized software packages specifically designed for serving ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f2d2d",
   "metadata": {},
   "source": [
    "It is important to note that this list is not exhaustive and there is more to consider when building machine learning applications. Nonetheless, it is undeniable that these four steps are critical for the success of a data science project that utilizes machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9d3a9",
   "metadata": {},
   "source": [
    "![Data Science Workflow](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_01/ds_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d395aa2",
   "metadata": {},
   "source": [
    "Ray has created dedicated libraries for each of the four ML-specific steps mentioned earlier. These libraries include Ray Datasets for data processing, Ray Train for distributed model training, Ray RLlib for reinforcement learning workloads, Ray Tune for efficient hyperparameter tuning, and Ray Serve for serving models. It is important to note that all of these libraries are distributed by design, as that is how Ray is built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faef52c",
   "metadata": {},
   "source": [
    "Additionally, it is important to consider that these steps are usually not completed separately but rather as part of a larger process. It is beneficial to have all relevant libraries working smoothly together and to have a uniform API throughout the data science process. The Ray AI Runtime (AIR) was designed with this in mind, providing a common runtime and API for experiments and the capability to expand workloads as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40b5e9",
   "metadata": {},
   "source": [
    "![Ray AIR](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_01/AIR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9918c5a",
   "metadata": {},
   "source": [
    "In this chapter, we will not be discussing the Ray AI Runtime API in depth (more on that can be found in Chapter 10). However, we can provide an overview of the components that contribute to it. Specifically, we will go through each of the DS libraries that make up Ray one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4312e12f",
   "metadata": {},
   "source": [
    "### Ray Data\n",
    "The first high-level library of Ray we talk about is called \"Ray Data\".\n",
    "This library contains a data structure aptly called `Dataset`, a multitude of\n",
    "connectors for loading data from various formats and systems,\n",
    "an API for transforming such datasets, a way to build data processing pipelines\n",
    "with them, and many integrations with other data processing frameworks.\n",
    "The `Dataset` abstraction builds on the powerful\n",
    "[Arrow framework](https://arrow.apache.org/).\n",
    "\n",
    "To use Ray Data, you need to install Arrow for Python, for instance by running\n",
    "`pip install pyarrow`.\n",
    "We'll now discuss a simple example that creates a distributed `Dataset` on your\n",
    "local Ray cluster from a Python data structure.\n",
    "Specifically, you'll create a dataset from a Python dictionary containing a\n",
    "string `name` and an integer-valued `data` for `10000` entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121d832a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pydantic.fields' has no attribute 'ModelField'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[1;32m      3\u001b[0m items \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(i), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: i} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m)]\n\u001b[0;32m----> 4\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m ds\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/data/read_api.py:111\u001b[0m, in \u001b[0;36mfrom_items\u001b[0;34m(items, parallelism)\u001b[0m\n\u001b[1;32m    109\u001b[0m builder \u001b[38;5;241m=\u001b[39m DelegatingBlockBuilder()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items[i : i \u001b[38;5;241m+\u001b[39m block_size]:\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m block \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m    113\u001b[0m blocks\u001b[38;5;241m.\u001b[39mappend(ray\u001b[38;5;241m.\u001b[39mput(block))\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/data/_internal/delegating_block_builder.py:25\u001b[0m, in \u001b[0;36mDelegatingBlockBuilder.add\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     check \u001b[38;5;241m=\u001b[39m ArrowBlockBuilder()\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mcheck\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     check\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_builder \u001b[38;5;241m=\u001b[39m ArrowBlockBuilder()\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/data/_internal/table_block.py:69\u001b[0m, in \u001b[0;36mTableBlockBuilder.add\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compact_if_needed()\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_uncompacted_size\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/data/_internal/size_estimator.py:23\u001b[0m, in \u001b[0;36mSizeEstimator.add\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mean\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_real_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/data/_internal/size_estimator.py:45\u001b[0m, in \u001b[0;36mSizeEstimator._real_size\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ray_initialized:\n\u001b[1;32m     44\u001b[0m     _ray_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     47\u001b[0m     ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mglobal_worker\u001b[38;5;241m.\u001b[39mget_serialization_context()\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;241m.\u001b[39mserialize(item)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;241m.\u001b[39mtotal_bytes\n\u001b[1;32m     50\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/_private/worker.py:2375\u001b[0m, in \u001b[0;36mput\u001b[0;34m(value, _owner)\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profiling\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mray.put\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2375\u001b[0m         object_ref \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowner_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserialize_owner_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ObjectStoreFullError:\n\u001b[1;32m   2377\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2378\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPut failed since the value was either too large or the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2379\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore was full of pinned objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2380\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/_private/worker.py:611\u001b[0m, in \u001b[0;36mWorker.put_object\u001b[0;34m(self, value, object_ref, owner_address)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m LOCAL_MODE:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    608\u001b[0m         object_ref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocal Mode does not support inserting with an ObjectRef\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 611\u001b[0m serialized_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_serialization_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mserialize(value)\n\u001b[1;32m    612\u001b[0m \u001b[38;5;66;03m# This *must* be the first place that we construct this python\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# ObjectRef because an entry with 0 local references is created when\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# the object is Put() in the core worker, expecting that this python\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m# reference will be created. If another reference is created and\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# removed before this one, it will corrupt the state in the\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# reference counter.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mObjectRef(\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mput_serialized_object_and_increment_local_ref(\n\u001b[1;32m    620\u001b[0m         serialized_value, object_ref\u001b[38;5;241m=\u001b[39mobject_ref, owner_address\u001b[38;5;241m=\u001b[39mowner_address\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m     skip_adding_local_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    624\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/_private/worker.py:537\u001b[0m, in \u001b[0;36mWorker.get_serialization_context\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    535\u001b[0m         context_map[job_id] \u001b[38;5;241m=\u001b[39m context_map\u001b[38;5;241m.\u001b[39mpop(JobID\u001b[38;5;241m.\u001b[39mnil())\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m         context_map[job_id] \u001b[38;5;241m=\u001b[39m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializationContext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context_map[job_id]\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/_private/serialization.py:135\u001b[0m, in \u001b[0;36mSerializationContext.__init__\u001b[0;34m(self, worker)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ObjectRefGenerator, (obj\u001b[38;5;241m.\u001b[39m_refs,)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_cloudpickle_reducer(\n\u001b[1;32m    132\u001b[0m     ObjectRefGenerator, object_ref_generator_reducer\n\u001b[1;32m    133\u001b[0m )\n\u001b[0;32m--> 135\u001b[0m \u001b[43mserialization_addons\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/util/serialization_addons.py:58\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(serialization_context)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;129m@DeveloperAPI\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(serialization_context):\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mregister_pydantic_serializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserialization_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     register_starlette_serializer(serialization_context)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rayenv/lib/python3.10/site-packages/ray/util/serialization_addons.py:21\u001b[0m, in \u001b[0;36mregister_pydantic_serializer\u001b[0;34m(serialization_context)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Pydantic's Cython validators are not serializable.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# https://github.com/cloudpipe/cloudpickle/issues/408\u001b[39;00m\n\u001b[1;32m     20\u001b[0m serialization_context\u001b[38;5;241m.\u001b[39m_register_cloudpickle_serializer(\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mpydantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelField\u001b[49m,\n\u001b[1;32m     22\u001b[0m     custom_serializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m o: {\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: o\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# outer_type_ is the original type for ModelFields,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# while type_ can be updated later with the nested type\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;66;03m# like int for List[int].\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype_\u001b[39m\u001b[38;5;124m\"\u001b[39m: o\u001b[38;5;241m.\u001b[39mouter_type_,\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_validators\u001b[39m\u001b[38;5;124m\"\u001b[39m: o\u001b[38;5;241m.\u001b[39mclass_validators,\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: o\u001b[38;5;241m.\u001b[39mmodel_config,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m: o\u001b[38;5;241m.\u001b[39mdefault,\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m: o\u001b[38;5;241m.\u001b[39mdefault_factory,\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequired\u001b[39m\u001b[38;5;124m\"\u001b[39m: o\u001b[38;5;241m.\u001b[39mrequired,\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malias\u001b[39m\u001b[38;5;124m\"\u001b[39m: o\u001b[38;5;241m.\u001b[39malias,\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: o\u001b[38;5;241m.\u001b[39mfield_info,\n\u001b[1;32m     35\u001b[0m     },\n\u001b[1;32m     36\u001b[0m     custom_deserializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m kwargs: pydantic\u001b[38;5;241m.\u001b[39mfields\u001b[38;5;241m.\u001b[39mModelField(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pydantic.fields' has no attribute 'ModelField'"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "items = [{\"name\": str(i), \"data\": i} for i in range(10000)]\n",
    "ds = ray.data.from_items(items)\n",
    "ds.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95752b",
   "metadata": {},
   "source": [
    "Great, now you have some rows, but what can you do with that data?\n",
    "The `Dataset` API bets heavily on functional programming, as it is very well suited\n",
    "for data transformations.\n",
    "Even though Python 3 made a point of hiding some of its functional programming\n",
    "capabilities, you're probably\n",
    "familiar with functionality such as `map`, `filter` and others.\n",
    "If not, it's easy enough to pick up.\n",
    "`map` takes each element of your dataset and transforms is into something\n",
    "else, in parallel.\n",
    "`filter` removes data points according to a boolean filter function.\n",
    "And the slightly more elaborate `flat_map` first maps values similarly to `map`,\n",
    "but then also \"flattens\" the result.\n",
    "For instance, if `map` would produce a list of lists, `flat_map` would flatten out\n",
    "the nested lists and give\n",
    "you just a list.\n",
    "Equipped with these three functional API calls, let's see how easily you can\n",
    "transform your dataset `ds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e008dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = ds.map(lambda x: x[\"data\"] ** 2)\n",
    "\n",
    "evens = squares.filter(lambda x: x % 2 == 0)\n",
    "evens.count()\n",
    "\n",
    "cubes = evens.flat_map(lambda x: [x, x**3])\n",
    "sample = cubes.take(10)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e42ee9",
   "metadata": {},
   "source": [
    "The drawback of `Dataset` transformations is that each step gets executed\n",
    "synchronously.\n",
    "In the above example this is a non-issue, but for complex tasks that e.g. mix\n",
    "reading files and processing data,\n",
    "you want an execution that can overlap the individual tasks.\n",
    "`DatasetPipeline` does exactly that.\n",
    "Let's rewrite the last example into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ds.window()\n",
    "result = pipe\\\n",
    "    .map(lambda x: x[\"data\"] ** 2)\\\n",
    "    .filter(lambda x: x % 2 == 0)\\\n",
    "    .flat_map(lambda x: [x, x**3])\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf7fd1",
   "metadata": {},
   "source": [
    "While there is much more that can be explored regarding Ray Datasets and its integration with certain data processing systems, we will have to postpone a more thorough discussion until Chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db60f0",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Next, we will examine the distributed training abilities of Ray through two libraries. The first library is specifically for reinforcement learning, while the second library is primarily focused on supervised learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a3296",
   "metadata": {},
   "source": [
    "### RL with Ray RLlib\n",
    "\n",
    "We will begin with discussing Ray RLlib for reinforcement learning, a library that utilizes either TensorFlow or PyTorch as its underlying machine learning framework. Both of these frameworks are highly compatible with each other, so you can use whichever one you prefer without sacrificing much in terms of functionality. In this book, we will provide examples using both TensorFlow and PyTorch to give you a comprehensive understanding of how to use Ray with either framework. In this chapter we'll work with TensorFlow, which you can install by simply running the command \"pip install tensorflow\" in your terminal. If you wanted to, you could equally well install PyTorch instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88c28e",
   "metadata": {},
   "source": [
    "RLlib provides a command line tool called `rllib` that can be easily used to run examples. It was already installed when you ran \"pip install 'ray[rllib]'\" earlier. While you will primarily use the Python API for more advanced examples in Chapter 4, this tool allows you to quickly try out RL experiments using RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a7dbc",
   "metadata": {},
   "source": [
    "We will consider a classic control problem in which we try to balance a pole on a cart. Imagine that the pole is attached to the cart at a joint and is subject to the force of gravity. The cart is able to move along a frictionless track and we can give it a push to the left or right with a fixed force. If we do this properly, the pole should remain upright. For each time step in which the pole does not fall, we receive a reward of 1. Our goal is to collect as many rewards as possible and we want to see if we can use a reinforcement learning algorithm to help us achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5432ed",
   "metadata": {},
   "source": [
    "![Cartpole Env](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_01/cartpole.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7c019",
   "metadata": {},
   "source": [
    "Our goal is to train a reinforcement learning agent that can perform two actions: pushing to the left or to the right, observe the consequences of these actions, and learn from the experience to maximize the reward. To achieve this using Ray RLlib, we can utilize a \"tuned example,\" which is a pre-set algorithm that works effectively for a specific problem. These examples can be easily run with a single command and RLlib offers a variety of them, which can be viewed by using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rllib example list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1aae96",
   "metadata": {},
   "source": [
    "An example that is available is called cartpole-ppo, which utilizes the PPO algorithm to solve the cartpole problem in the CartPole-v1 environment from OpenAI Gym (https://gymnasium.farama.org/environments/classic_control/cart_pole/). You can access the configuration of this example by entering `rllib example get cartpole-ppo` in the command line. This will first download the example file from GitHub and then display the configuration, which is written in YAML format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rllib example get cartpole-ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e923a2fe",
   "metadata": {},
   "source": [
    "While the specific details of the configuration file are not relevant at this time, it is important to note that you must include the Cartpole-v1 environment and the necessary RL configuration for the training process to function properly. You do not need any special equipment to run this configuration, and it should only take a few minutes to complete. In order to train this example, you will need to install the PyGame dependency by using the command `pip install pygame`, and then simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rllib example run cartpole-ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968bbd12",
   "metadata": {},
   "source": [
    "If you run this, RLlib creates a named experiment and logs important metrics such as\n",
    "the reward, or the `episode_reward_mean` for you. In the output of the training run,\n",
    "you should also see information about the machine (`loc`, meaning host name and\n",
    "port), as well as the status of your training runs. If your run is `TERMINATED`, but you’ve\n",
    "never seen a successfully `RUNNING` experiment in the log, something must have gone\n",
    "wrong. Here’s a sample snippet of a training run:\n",
    "\n",
    "```{text}\n",
    "+-----------------------------+----------+----------------+\n",
    "| Trial name | status | loc |\n",
    "|-----------------------------+----------+----------------|\n",
    "| PPO_CartPole-v0_9931e_00000 | RUNNING | 127.0.0.1:8683 |\n",
    "+-----------------------------+----------+----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc646f",
   "metadata": {},
   "source": [
    "When the training run finishes and things went well, you should see the following\n",
    "output:\n",
    "\n",
    "```{text}\n",
    "Your training finished.\n",
    "Best available checkpoint for each trial:\n",
    "<checkpoint-path>/checkpoint_<number>\n",
    "\n",
    "You can now evaluate your trained algorithm from any checkpoint,\n",
    "e.g. by running:\n",
    "\n",
    "╭─────────────────────────────────────────────────────────────────────────╮\n",
    "│ rllib evaluate <checkpoint-path>/checkpoint_<number> --algo PPO         │\n",
    "╰─────────────────────────────────────────────────────────────────────────╯\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d054d",
   "metadata": {},
   "source": [
    "Your local Ray checkpoint folder is `~/ray-results` by default. For the training\n",
    "configuration we used, your `<checkpoint-path>` should be of the form \n",
    "`~/ray_results/cartpole-ppo/PPO_CartPole-v1_<experiment_id>`. During training\n",
    "procedure, your intermediate and final model checkpoints get generated into this\n",
    "folder.\n",
    "\n",
    "To evaluate the performance of your trained RL algorithm, you can now evaluate it\n",
    "from checkpoint by copying the command the previous example training run printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a631a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rllib evaluate <checkpoint-path>/checkpoint_<number> --algo PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e553bda",
   "metadata": {},
   "source": [
    "Executing this command will display the rewards obtained by the RL algorithm you trained in the `CartPole-v1` environment. There is a lot more that can be done with RLlib, which will be discussed further in Chapter 4. The purpose of this example was to demonstrate how easy it is to begin using RLlib and the `rllib` command line tool through the use of the example and evaluate commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa244c",
   "metadata": {},
   "source": [
    "#### Ray Train\n",
    "\n",
    "If you are interested in using Ray for supervised learning, rather than just reinforcement learning, you can use the Ray Train library. However, we do not have enough expertise with frameworks like TensorFlow to provide a detailed example of how to use Ray Train at this time. If you want to learn more about distributed training, you can move on to Chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35125e",
   "metadata": {},
   "source": [
    "### Ray Tune\n",
    "Naming things is hard, but the Ray team hit the spot with _Ray Tune_, which you can\n",
    "use to tune all\n",
    "sorts of parameters.\n",
    "Specifically, it was built to find good hyperparameters for machine learning models.\n",
    "The typical setup is as follows:\n",
    "\n",
    "- You want to run an extremely computationally expensive training function. In ML it's not uncommon\n",
    "  to run training procedures that take days, if not weeks, but let's say you're dealing with just a couple of minutes.\n",
    "- As result of training, you compute a so-called objective function. Usually you either want to maximize\n",
    "  your gains or minimize your losses in terms of performance of your experiment.\n",
    "- The tricky bit is that your training function might depend on certain parameters,\n",
    "  hyperparameters, that influence the value of your objective function.\n",
    "- You may have a hunch what individual hyperparameters should be, but tuning them all can be difficult.\n",
    "  Even if you can restrict these parameters to a sensible range, it's usually prohibitive to test a wide\n",
    "  range of combinations. Your training function is simply too expensive.\n",
    "\n",
    "What can you do to efficiently sample hyperparameters and get \"good enough\" results on your objective?\n",
    "The field concerned with solving this problem is called _hyperparameter optimization_ (HPO), and Ray Tune has\n",
    "an enormous suite of algorithms for tackling it.\n",
    "Let's look at a first example of Ray Tune used for the situation we just explained.\n",
    "The focus is yet again on Ray and its API, and not on a specific ML task (which we simply simulate for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def training_function(config):\n",
    "    x, y = config[\"x\"], config[\"y\"]\n",
    "    time.sleep(10)\n",
    "    score = objective(x, y)\n",
    "    tune.report(score=score)\n",
    "\n",
    "\n",
    "def objective(x, y):\n",
    "    return math.sqrt((x**2 + y**2)/2)\n",
    "\n",
    "\n",
    "result = tune.run(\n",
    "    training_function,\n",
    "    config={\n",
    "        \"x\": tune.grid_search([-1, -.5, 0, .5, 1]),\n",
    "        \"y\": tune.grid_search([-1, -.5, 0, .5, 1])\n",
    "    })\n",
    "\n",
    "print(result.get_best_config(metric=\"score\", mode=\"min\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e790e45",
   "metadata": {},
   "source": [
    "Note how the output of this run is structurally similar to what you’ve\n",
    "seen in the RLlib example. That’s no coincidence, as RLlib (like many other Ray\n",
    "libraries) uses Ray Tune under the hood. If you look closely, you will see `PENDING`\n",
    "runs that wait for execution, as well as `RUNNING` and `TERMINATED` runs. Tune takes care\n",
    "of selecting, scheduling, and executing your training runs for you automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f30412",
   "metadata": {},
   "source": [
    "This Tune example aims to identify the optimal values for parameters x and y for a training_function with the goal of minimizing a particular objective. Although the objective function may seem complicated as it involves calculating the sum of the squares of x and y, all of the values will be non-negative. Therefore, the lowest value is achieved when x and y are both equal to 0, resulting in an evaluation of 0 for the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3298b6",
   "metadata": {},
   "source": [
    "We do a so-called grid search over all possible parameter combinations. As we explicitly\n",
    "pass in five possible values for both x and y that’s a total of 25 combinations\n",
    "that get fed into the training function. These combinations are evaluated through the training function, which includes a 10 second sleep time. Without Ray's ability to parallelize the process, testing all of these combinations would take more than four minutes. However, on my laptop, this experiment only takes around 35 seconds to complete. The duration may vary depending on the device used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab1adf",
   "metadata": {},
   "source": [
    "If each training run took several hours, and there were 20 hyperparameters to consider instead of just two, it would not be practical to use grid search. This is especially true if you do not have a good idea of what range the parameters should be in. In these cases, you will need to use more advanced HPO methods like those offered by Ray Tune, which we will discuss in Chapter 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b0c80",
   "metadata": {},
   "source": [
    "### Ray Serve\n",
    "\n",
    "The last of Ray's high-level libraries we'll discuss specializes on model serving and is simply called _Ray Serve_.\n",
    "To see an example of it in action, you need a trained ML model to serve.\n",
    "Luckily, nowadays you can find many interesting models on the internet that have already been trained for you.\n",
    "For instance, _Hugging Face_ has a variety of models available for you to download directly in Python.\n",
    "The model we'll use is a language model called _GPT-2_ that takes text as input and produces text to\n",
    "continue or complete the input.\n",
    "For example, you can prompt a question and GPT-2 will try to complete it.\n",
    "\n",
    "Serving such a model is a good way to make it accessible.\n",
    "You may not now how to load and run a TensorFlow model on your computer, but you do now how\n",
    "to ask a question in plain English.\n",
    "Model serving hides the implementation details of a solution and lets users focus on providing\n",
    "inputs and understanding outputs of a model.\n",
    "\n",
    "To proceed, make sure to run `pip install transformers` to install the Hugging Face library\n",
    "that has the model we want to use.\n",
    "With that we can now import and start an instance of Ray's `serve` library, load and deploy a GPT-2\n",
    "model and ask it for the meaning of life, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "\n",
    "serve.start()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "def model(request):\n",
    "    language_model = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "    query = request.query_params[\"query\"]\n",
    "    return language_model(query, max_length=100)\n",
    "\n",
    "\n",
    "model.deploy()\n",
    "\n",
    "query = \"What's the meaning of life?\"\n",
    "response = requests.get(f\"http://localhost:8000/model?query={query}\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65135823",
   "metadata": {},
   "source": [
    "In Chapter 9, you will be taught how to correctly implement models in various situations. However, for now, I recommend that you experiment with this example and try different queries. If you repeatedly run the last two lines of code, you will get practically different answers every time. Here is a poetic statement that I queried on my computer, which has been slightly edited for younger readers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071a0c9",
   "metadata": {},
   "source": [
    "```{text}\n",
    "[{\n",
    "\"generated_text\": \"What's the meaning of life?\\n\\n\n",
    "Is there one way or another of living?\\n\\n\n",
    "How does it feel to be trapped in a relationship?\\n\\n\n",
    "How can it be changed before it's too late?\n",
    "What did we call it in our time?\\n\\n\n",
    "Where do we fit within this world and what are we going to live for?\\n\\n\n",
    "My life as a person has been shaped by the love I've received from others.\"\n",
    "}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab453063",
   "metadata": {},
   "source": [
    "This is the end of our overview of the data science libraries within the second layer of Ray. These libraries, which we have discussed in this chapter, are all based on the Ray Core API. It is fairly simple to create new extensions for Ray, and there are a few more that we are unable to cover in this book. For example, the Ray Workflows library (https://docs.ray.io/en/latest/workflows/index.html) allows users to define and run long-term applications using Ray. Before we conclude this chapter, let's briefly examine the third layer of Ray, which is the expanding ecosystem surrounding the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0926a41a",
   "metadata": {},
   "source": [
    "## The Ray Ecosystem\n",
    "\n",
    "Ray's libraries are powerful and should be discussed in more detail in the book. Although they are very useful for data science work, we don't want to give the impression that they are the only thing you need from now on. The most successful frameworks are those that work well with other solutions and ideas. It is better to concentrate on your strengths and use other tools to fill any gaps in your solution, which Ray does well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b073dd4",
   "metadata": {},
   "source": [
    "Throughout the book, we will be discussing various libraries that have been built on top of Ray. Additionally, Ray has integrations with existing tools like Spark, Dask, and Pandas. For instance, you can use Ray Datasets, a data loading and compute library, with your existing project that utilizes data processing engines like Spark or Dask. Additionally, you can run the entire Dask ecosystem on a Ray cluster with the Dask-on-Ray scheduler or use the Spark on Ray project to integrate your Spark workloads with Ray. The Modin project also offers a distributed replacement for Pandas dataframes that utilizes Ray or Dask as the distributed execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba65e3",
   "metadata": {},
   "source": [
    "Ray's approach is to integrate with various tools rather than trying to replace them, while still providing access to its own native library called Ray Datasets. This will be explored in more detail later in Chapter 11. It's noteworthy that many of the Ray libraries have the ability to seamlessly integrate with other tools as backends, often by creating common interfaces rather than establishing new standards. These interfaces enable you to perform tasks in a distributed manner, something that many of the backends may not offer or may not offer to the same degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e88155",
   "metadata": {},
   "source": [
    "For example, Ray RLlib and Train both utilize the capabilities of TensorFlow and PyTorch. Additionally, Ray Tune allows for the use of a variety of HPO tools, such as Hyperopt, Optuna, Nevergrad, Ax, and SigOpt, among others. These tools are not automatically distributed, but Tune brings them together in a unified interface for distributed tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7279b0",
   "metadata": {},
   "source": [
    "![Ray Layers](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_01/ray_layers.png)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
